{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로드\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "from reviewclassifiermodel import reviewClassifierModel\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드함수\n",
    "def load_data(csvfile1,csvfile2):                           # csv 파일 읽기\n",
    "    trainDF = pd.read_csv(csvfile1, usecols=[1, 2, 4])      # 필요한 컬럼 추출\n",
    "    testDF = pd.read_csv(csvfile2, usecols=[1, 2, 4])\n",
    "    return trainDF, testDF                                  # 리턴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 인코딩 함수\n",
    "def data_encoding(DF):\n",
    "    labelCD = DF.Aspect.unique().tolist()                   # Aspect 컬럼의 유니크 값 리스트 \n",
    "    DF['Aspect'] = DF['Aspect'].map(lambda x: labelCD.index(x))         # 다중 분류 라벨링 인코딩\n",
    "    DF.loc[DF['SentimentPolarity'] == -1, 'SentimentPolarity'] = 0      # 2진 분류 인코딩\n",
    "    return DF, labelCD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 만드는 함수\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()                                     # Counter 인스턴스 생성\n",
    "    for tokens in corpus:                                   # 입력받은 corpus로 카운터 모델 초기화\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens.copy()                           \n",
    "    for token, count in counter.most_common(n_vocab):       # 상위 중복 언어 단어사전에 추가\n",
    "        vocab.append(token)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩함수\n",
    "def pad_sequences(sequences, max_length, pad_value):\n",
    "    result = list()\n",
    "    for sequence in sequences:                              \n",
    "        sequence = sequence[:max_length]                    # max_length 만큼 자르기\n",
    "        pad_length = max_length - len(sequence)             # max_length보다 단어가 적다면\n",
    "        padded_sequence = sequence + [pad_value] * pad_length   # 정해진 수 채우기\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 인코딩 함수\n",
    "def encoding_ids(token_to_id, tokens, unk_id):\n",
    "    return [\n",
    "        [token_to_id.get(token, unk_id) for token in review] for review in tokens\n",
    "    ]   # 자연어 정수화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습함수\n",
    "def model_train(model, datasets, cl_criterion, bn_criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)                    # 인풋데이터\n",
    "        cl_labels = labels[:, 0].to(device)                 # 라벨 다중분류\n",
    "        bn_labels = labels[:, 1].to(device).float()         # 라벨 2진분류  (float형)\n",
    "\n",
    "        # Forward pass\n",
    "        classesd, logits = model(input_ids)\n",
    "\n",
    "        # Calculate losses\n",
    "        loss_cl = cl_criterion(classesd, cl_labels)         # \n",
    "        loss_bn = bn_criterion(logits.squeeze(), bn_labels) # \n",
    "        loss = loss_cl + loss_bn\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 함수\n",
    "\n",
    "def model_test(model, datasets, cl_criterion, bn_criterion, device, epoch, results_df):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    cl_score = []\n",
    "    bn_score = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for step, (input_ids, labels) in enumerate(datasets):\n",
    "            input_ids = input_ids.to(device)\n",
    "            cl_labels = labels[:, 0].to(device).long()\n",
    "            bn_labels = labels[:, 1].to(device).float() \n",
    "\n",
    "            # Forward pass\n",
    "            classesd, logits = model(input_ids)\n",
    "\n",
    "            # Calculate losses\n",
    "            loss_cl = cl_criterion(classesd, cl_labels)\n",
    "            loss_bn = bn_criterion(logits.squeeze(), bn_labels)\n",
    "            loss = loss_cl + loss_bn\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Calculate class accuracy\n",
    "            cl_predictions = torch.argmax(torch.softmax(classesd, dim=1), dim=1)  # 다중 클래스 예측\n",
    "            cl_score.extend(cl_predictions.eq(cl_labels).cpu().numpy())  # 정확도 계산\n",
    "            \n",
    "            # Calculate binary accuracy\n",
    "            bn_predictions = (torch.sigmoid(logits) > 0.5).int().squeeze()  # 이진 예측\n",
    "            bn_score.extend(bn_predictions.eq(bn_labels.int()).cpu().numpy())  # 정확도 계산\n",
    "        \n",
    "        # 정확도 계산\n",
    "        cl_accuracy = np.mean(cl_score)\n",
    "        bn_accuracy = np.mean(bn_score)\n",
    "        \n",
    "        print(f'Epoch {epoch} - Val Loss: {np.mean(losses)}, bn_score Val Accuracy: {bn_accuracy}, cl_score Val Accuracy: {cl_accuracy}')\n",
    "\n",
    "        # 결과를 DataFrame에 추가\n",
    "        results_df.loc[epoch] = [np.mean(losses), bn_accuracy, cl_accuracy]  # 각 에포크의 결과를 DataFrame에 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행함수\n",
    "def main():\n",
    "    N_VOCAB = 5000\n",
    "    MAX_LENGTH = 38\n",
    "    EPOCHS = 5\n",
    "    INTERVAL = 500\n",
    "    BATCH_SIZE = 64\n",
    "    LR = 0.001\n",
    "    special_tokens = ['<pad>', '<unk>']\n",
    "\n",
    "    trainDF, testDF = load_data('./data/train.csv','./data/test.csv')\n",
    "\n",
    "    trainDF, aspectCD = data_encoding(trainDF)\n",
    "    testDF, _ = data_encoding(testDF)\n",
    "\n",
    "    ## 토큰화 및 불용어 처리 ------------------------------------------------------------------------------------------------------------\n",
    "    punc=string.punctuation\n",
    "\n",
    "    for p in punc:\n",
    "        trainDF['SentimentText'] = trainDF['SentimentText'].str.replace(p, '')\n",
    "        testDF['SentimentText']=testDF['SentimentText'].str.replace(p,'')\n",
    "\n",
    "    m=re.compile('[^ ㄱ-ㅣ가-힣]+')     # 한글만 남김\n",
    "\n",
    "    trainDF['SentimentText']=trainDF['SentimentText'].apply(lambda x: m.sub(' ', x))\n",
    "    testDF['SentimentText']=testDF['SentimentText'].apply(lambda x: m.sub(' ', x))\n",
    "\n",
    "    stop_word='./data/stopwords.txt'    # 잘 사용하지 않는 단어 \n",
    "\n",
    "    with open(stop_word, 'r', encoding='utf-8') as f:\n",
    "        stop_words = [line.strip() for line in f]\n",
    "    \n",
    "\n",
    "    tokenizer = Okt()\n",
    "    train_tokens = [[token for token in tokenizer.morphs(text) if token not in stop_words] for text in trainDF['SentimentText']]\n",
    "    test_tokens = [[token for token in tokenizer.morphs(text) if token not in stop_words] for text in testDF['SentimentText']]\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------\n",
    "    vocab = build_vocab(train_tokens, N_VOCAB, special_tokens)\n",
    "    token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "    id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "    save_vocab(vocab,'./data/cosmetic_vocab.pkl')\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    vocab = build_vocab(train_tokens, N_VOCAB, special_tokens)\n",
    "    token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "    id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "    pad_id = token_to_id['<pad>']\n",
    "    unk_id = token_to_id['<unk>']\n",
    "    train_ids = encoding_ids(token_to_id, train_tokens, unk_id)     #정수화\n",
    "    test_ids = encoding_ids(token_to_id, test_tokens, unk_id)       #정수화\n",
    "    train_ids = pad_sequences(train_ids, MAX_LENGTH, pad_id)\n",
    "    test_ids = pad_sequences(test_ids, MAX_LENGTH, pad_id)\n",
    "\n",
    "    # 텐서화\n",
    "    train_ids = torch.tensor(train_ids, dtype=torch.long)\n",
    "    test_ids = torch.tensor(test_ids, dtype=torch.long)\n",
    "\n",
    "    # 레이블 텐서화\n",
    "    train_labels = torch.tensor(list(zip(trainDF['Aspect'].values, trainDF['SentimentPolarity'].values)), dtype=torch.long)\n",
    "    test_labels = torch.tensor(list(zip(testDF['Aspect'].values, testDF['SentimentPolarity'].values)), dtype=torch.float32)\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    train_dataset = TensorDataset(train_ids, train_labels)\n",
    "    test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 모델 초기화\n",
    "    n_vocab = len(token_to_id)  # 어휘 크기 계산\n",
    "    hidden_dim = 64 \n",
    "    embedding_dim = 128\n",
    "    n_layers = 2\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    classifier = reviewClassifierModel(\n",
    "        n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_classes=len(aspectCD), n_layers=n_layers\n",
    "    ).to(device)\n",
    "\n",
    "    # 손실 함수 및 최적화기 설정\n",
    "    cl_criterion = nn.NLLLoss().to(device)          \n",
    "    bn_criterion = nn.BCEWithLogitsLoss().to(device)    \n",
    "    optimizer = optim.RMSprop(classifier.parameters(), lr=LR)\n",
    "\n",
    " \n",
    "    # 결과를 저장할 DataFrame 생성\n",
    "    results_df = pd.DataFrame(columns=['Val Loss', 'bn_score Val Accuracy', 'cl_score Val Accuracy'])\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_train(classifier, train_loader, cl_criterion, bn_criterion, optimizer, device, INTERVAL)\n",
    "        model_test(classifier, test_loader, cl_criterion, bn_criterion, device, epoch, results_df)  # DataFrame 전달\n",
    "\n",
    "        # 모델 저장 (에포크 번호 추가)\n",
    "        model_save_path = f'./saved_model/review_classifier_BATCH_8_epoch_{epoch}.pth'  # 에포크 번호 포함\n",
    "        torch.save(classifier.state_dict(), model_save_path)\n",
    "        print(f'Model saved at {model_save_path}')\n",
    "\n",
    "    # 결과 DataFrame 저장\n",
    "    results_df.to_csv('./saved_model/evaluation_results.csv', index=True)\n",
    "    print(\"평가 결과가 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = './data/Training/'\n",
    "TEST_PATH='./data/Validation/'\n",
    "# 여러 폴더 경로를 리스트로 저장\n",
    "train_folder_paths = os.listdir(TRAIN_PATH)\n",
    "test_folder_paths = os.listdir(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 데이터프레임 리스트 생성\n",
    "dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 각 폴더 내의 JSON 파일을 읽어와 데이터프레임으로 변환\n",
    "def json2df(folder_paths,csv_paths):\n",
    "    for folder_path in folder_paths:\n",
    "        FOLDER_PATH = TRAIN_PATH+folder_path\n",
    "        print(f\"Processing folder: {folder_path}\")\n",
    "        \n",
    "        # 폴더 내의 모든 JSON 파일 리스트\n",
    "        json_files = [file for file in os.listdir(FOLDER_PATH) if file.endswith('.json')]\n",
    "\n",
    "        for file in json_files:\n",
    "            file_path = os.path.join(FOLDER_PATH, file)\n",
    "            print(f\"Loading file: {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # 파일 내용 확인 및 데이터프레임으로 변환\n",
    "                    if data:\n",
    "                        # Aspects만 추출\n",
    "                        for review in data:\n",
    "                            aspects = pd.json_normalize(review.get('Aspects'))\n",
    "                            dataframes.append(aspects)\n",
    "                    else:\n",
    "                        print(f\"No data found in {file}\")\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error loading {file}: Invalid JSON\")\n",
    "\n",
    "    # 데이터프레임 결합\n",
    "    if dataframes:\n",
    "        final_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "        print(final_dataframe)\n",
    "    else:\n",
    "        print(\"No valid dataframes to concatenate.\")\n",
    "        \n",
    "    final_dataframe.to_csv(csv_paths)\n",
    "\n",
    "    return final_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 2-1\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(1).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(10).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(100).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(101).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(102).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(103).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(104).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(105).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(106).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(107).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(108).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(109).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(11).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(110).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(111).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(112).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(113).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(114).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(115).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(116).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(117).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(118).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(119).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(12).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(120).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(13).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(14).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(15).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(16).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(17).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(18).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(19).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(2).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(20).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(21).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(22).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(23).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(24).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(25).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(26).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(27).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(28).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(29).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(3).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(30).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(31).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(32).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(33).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(34).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(35).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(36).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(37).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(38).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(39).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(4).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(40).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(41).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(42).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(43).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(44).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(45).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(46).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(47).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(48).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(49).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(5).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(50).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(51).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(52).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(53).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(54).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(55).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(56).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(57).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(58).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(59).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(6).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(60).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(61).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(62).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(63).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(64).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(65).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(66).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(67).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(68).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(69).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(7).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(70).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(71).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(72).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(73).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(74).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(75).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(76).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(77).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(78).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(79).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(8).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(80).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(81).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(82).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(83).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(84).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(85).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(86).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(87).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(88).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(89).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(9).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(90).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(91).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(92).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(93).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(94).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(95).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(96).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(97).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(98).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(99).json\n",
      "Processing folder: 2-2\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(1).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(10).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(100).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(101).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(102).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(103).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(104).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(105).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(106).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(107).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(108).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(109).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(11).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(110).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(111).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(112).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(12).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(13).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(14).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(15).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(16).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(17).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(18).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(19).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(2).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(20).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(21).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(22).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(23).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(24).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(25).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(26).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(27).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(28).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(29).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(3).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(30).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(31).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(32).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(33).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(34).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(35).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(36).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(37).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(38).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(39).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(4).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(40).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(41).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(42).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(43).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(44).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(45).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(46).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(47).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(48).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(49).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(5).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(50).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(51).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(52).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(53).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(54).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(55).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(56).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(57).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(58).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(59).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(6).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(60).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(61).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(62).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(63).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(64).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(65).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(66).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(67).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(68).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(69).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(7).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(70).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(71).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(72).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(73).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(74).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(75).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(76).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(77).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(78).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(79).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(8).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(80).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(81).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(82).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(83).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(84).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(85).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(86).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(87).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(88).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(89).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(9).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(90).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(91).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(92).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(93).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(94).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(95).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(96).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(97).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(98).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(99).json\n",
      "Processing folder: 2-3\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(1).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(10).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(11).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(12).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(13).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(14).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(15).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(16).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(17).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(18).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(19).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(2).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(20).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(21).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(22).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(23).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(24).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(25).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(26).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(27).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(28).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(29).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(3).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(30).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(31).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(32).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(33).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(34).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(35).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(36).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(37).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(38).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(39).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(4).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(40).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(41).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(42).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(43).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(44).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(45).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(46).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(47).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(48).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(49).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(5).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(50).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(51).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(52).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(53).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(54).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(55).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(56).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(57).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(58).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(59).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(6).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(60).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(61).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(62).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(63).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(64).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(65).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(66).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(67).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(68).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(69).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(7).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(70).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(71).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(72).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(73).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(74).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(75).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(76).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(77).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(78).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(79).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(8).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(80).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(81).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(82).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(83).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(84).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(85).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(86).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(87).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(88).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(89).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(9).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(90).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(91).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(92).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(93).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(94).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(95).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(96).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(97).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(98).json\n",
      "Processing folder: 2-4\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(1).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(10).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(11).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(12).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(13).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(14).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(15).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(16).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(17).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(18).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(19).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(2).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(20).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(21).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(22).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(23).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(24).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(25).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(26).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(27).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(28).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(29).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(3).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(30).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(4).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(5).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(6).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(7).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(8).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(9).json\n",
      "         Aspect         SentimentText SentimentWord SentimentPolarity\n",
      "0          유통기한            유통기한도 넉넉하고             2                 1\n",
      "1          제품구성    구성도 많아서 선물 하기 좋네요.             5                 1\n",
      "2          제품구성                구성도알차고             1                 1\n",
      "3       보습력/수분감             촉촉하고너무좋아용             1                 1\n",
      "4            용량   대용량으로 넉넉하게 사용할 수 있고             5                 1\n",
      "...         ...                   ...           ...               ...\n",
      "115831  편의성/활용성     간편하게 하나만 발라도 되어서              4                 1\n",
      "115832        향            향기도 끝내줍니다.             2                 1\n",
      "115833        향             좋아하는 향이라              2                 1\n",
      "115834       가격  할인이 없어서 비싸게 처음 구매했어요             5                -1\n",
      "115835        향             향이 많이 진해서             3                -1\n",
      "\n",
      "[115836 rows x 4 columns]\n",
      "Processing folder: 2-1\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(1).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(10).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(100).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(101).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(102).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(103).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(104).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(105).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(106).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(107).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(108).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(109).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(11).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(110).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(111).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(112).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(113).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(114).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(115).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(116).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(117).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(118).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(119).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(12).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(120).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(13).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(14).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(15).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(16).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(17).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(18).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(19).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(2).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(20).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(21).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(22).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(23).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(24).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(25).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(26).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(27).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(28).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(29).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(3).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(30).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(31).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(32).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(33).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(34).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(35).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(36).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(37).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(38).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(39).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(4).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(40).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(41).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(42).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(43).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(44).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(45).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(46).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(47).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(48).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(49).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(5).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(50).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(51).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(52).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(53).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(54).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(55).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(56).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(57).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(58).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(59).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(6).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(60).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(61).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(62).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(63).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(64).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(65).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(66).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(67).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(68).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(69).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(7).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(70).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(71).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(72).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(73).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(74).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(75).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(76).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(77).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(78).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(79).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(8).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(80).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(81).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(82).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(83).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(84).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(85).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(86).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(87).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(88).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(89).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(9).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(90).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(91).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(92).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(93).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(94).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(95).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(96).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(97).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(98).json\n",
      "Loading file: ./data/Training/2-1\\2-1.스킨케어(99).json\n",
      "Processing folder: 2-2\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(1).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(10).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(100).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(101).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(102).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(103).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(104).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(105).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(106).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(107).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(108).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(109).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(11).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(110).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(111).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(112).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(12).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(13).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(14).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(15).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(16).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(17).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(18).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(19).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(2).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(20).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(21).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(22).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(23).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(24).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(25).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(26).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(27).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(28).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(29).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(3).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(30).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(31).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(32).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(33).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(34).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(35).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(36).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(37).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(38).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(39).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(4).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(40).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(41).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(42).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(43).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(44).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(45).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(46).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(47).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(48).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(49).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(5).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(50).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(51).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(52).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(53).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(54).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(55).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(56).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(57).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(58).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(59).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(6).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(60).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(61).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(62).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(63).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(64).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(65).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(66).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(67).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(68).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(69).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(7).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(70).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(71).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(72).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(73).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(74).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(75).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(76).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(77).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(78).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(79).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(8).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(80).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(81).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(82).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(83).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(84).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(85).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(86).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(87).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(88).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(89).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(9).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(90).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(91).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(92).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(93).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(94).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(95).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(96).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(97).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(98).json\n",
      "Loading file: ./data/Training/2-2\\2-2.헤어바디케어(99).json\n",
      "Processing folder: 2-3\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(1).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(10).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(11).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(12).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(13).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(14).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(15).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(16).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(17).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(18).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(19).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(2).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(20).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(21).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(22).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(23).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(24).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(25).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(26).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(27).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(28).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(29).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(3).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(30).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(31).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(32).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(33).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(34).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(35).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(36).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(37).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(38).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(39).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(4).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(40).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(41).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(42).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(43).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(44).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(45).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(46).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(47).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(48).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(49).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(5).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(50).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(51).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(52).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(53).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(54).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(55).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(56).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(57).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(58).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(59).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(6).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(60).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(61).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(62).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(63).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(64).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(65).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(66).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(67).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(68).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(69).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(7).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(70).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(71).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(72).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(73).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(74).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(75).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(76).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(77).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(78).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(79).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(8).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(80).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(81).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(82).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(83).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(84).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(85).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(86).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(87).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(88).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(89).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(9).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(90).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(91).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(92).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(93).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(94).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(95).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(96).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(97).json\n",
      "Loading file: ./data/Training/2-3\\2-3.메이크업뷰티소품(98).json\n",
      "Processing folder: 2-4\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(1).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(10).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(11).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(12).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(13).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(14).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(15).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(16).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(17).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(18).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(19).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(2).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(20).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(21).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(22).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(23).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(24).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(25).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(26).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(27).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(28).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(29).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(3).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(30).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(4).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(5).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(6).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(7).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(8).json\n",
      "Loading file: ./data/Training/2-4\\2-4.남성화장품(9).json\n",
      "         Aspect         SentimentText SentimentWord SentimentPolarity\n",
      "0          유통기한            유통기한도 넉넉하고             2                 1\n",
      "1          제품구성    구성도 많아서 선물 하기 좋네요.             5                 1\n",
      "2          제품구성                구성도알차고             1                 1\n",
      "3       보습력/수분감             촉촉하고너무좋아용             1                 1\n",
      "4            용량   대용량으로 넉넉하게 사용할 수 있고             5                 1\n",
      "...         ...                   ...           ...               ...\n",
      "231667  편의성/활용성     간편하게 하나만 발라도 되어서              4                 1\n",
      "231668        향            향기도 끝내줍니다.             2                 1\n",
      "231669        향             좋아하는 향이라              2                 1\n",
      "231670       가격  할인이 없어서 비싸게 처음 구매했어요             5                -1\n",
      "231671        향             향이 많이 진해서             3                -1\n",
      "\n",
      "[231672 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>SentimentWord</th>\n",
       "      <th>SentimentPolarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>유통기한</td>\n",
       "      <td>유통기한도 넉넉하고</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>제품구성</td>\n",
       "      <td>구성도 많아서 선물 하기 좋네요.</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>제품구성</td>\n",
       "      <td>구성도알차고</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>보습력/수분감</td>\n",
       "      <td>촉촉하고너무좋아용</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>용량</td>\n",
       "      <td>대용량으로 넉넉하게 사용할 수 있고</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231667</th>\n",
       "      <td>편의성/활용성</td>\n",
       "      <td>간편하게 하나만 발라도 되어서</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231668</th>\n",
       "      <td>향</td>\n",
       "      <td>향기도 끝내줍니다.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231669</th>\n",
       "      <td>향</td>\n",
       "      <td>좋아하는 향이라</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231670</th>\n",
       "      <td>가격</td>\n",
       "      <td>할인이 없어서 비싸게 처음 구매했어요</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231671</th>\n",
       "      <td>향</td>\n",
       "      <td>향이 많이 진해서</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231672 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Aspect         SentimentText SentimentWord SentimentPolarity\n",
       "0          유통기한            유통기한도 넉넉하고             2                 1\n",
       "1          제품구성    구성도 많아서 선물 하기 좋네요.             5                 1\n",
       "2          제품구성                구성도알차고             1                 1\n",
       "3       보습력/수분감             촉촉하고너무좋아용             1                 1\n",
       "4            용량   대용량으로 넉넉하게 사용할 수 있고             5                 1\n",
       "...         ...                   ...           ...               ...\n",
       "231667  편의성/활용성     간편하게 하나만 발라도 되어서              4                 1\n",
       "231668        향            향기도 끝내줍니다.             2                 1\n",
       "231669        향             좋아하는 향이라              2                 1\n",
       "231670       가격  할인이 없어서 비싸게 처음 구매했어요             5                -1\n",
       "231671        향             향이 많이 진해서             3                -1\n",
       "\n",
       "[231672 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json2df(train_folder_paths,'./data/train.csv')\n",
    "json2df(test_folder_paths,'./data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 4.460061073303223\n",
      "Train Loss 500 : 3.967137147328573\n",
      "Train Loss 1000 : 3.942793405020273\n",
      "Train Loss 1500 : 3.8902804163755533\n",
      "Train Loss 2000 : 3.862048703631659\n",
      "Train Loss 2500 : 3.8382910857530463\n",
      "Train Loss 3000 : 3.8215174782240404\n",
      "Train Loss 3500 : 3.811500640161989\n",
      "Train Loss 4000 : 3.801233565500455\n",
      "Train Loss 4500 : 3.7891674226084118\n",
      "Train Loss 5000 : 3.778484753050153\n",
      "Train Loss 5500 : 3.7730548696027326\n",
      "Train Loss 6000 : 3.766986193646592\n",
      "Train Loss 6500 : 3.7609858833043655\n",
      "Train Loss 7000 : 3.752698811168859\n",
      "Train Loss 7500 : 3.747951140198417\n",
      "Train Loss 8000 : 3.742526858288889\n",
      "Train Loss 8500 : 3.7384936557967667\n",
      "Train Loss 9000 : 3.7298116592841204\n",
      "Train Loss 9500 : 3.7187446440010246\n",
      "Train Loss 10000 : 3.7093870610001685\n",
      "Train Loss 10500 : 3.696035494635008\n",
      "Train Loss 11000 : 3.6796381441032593\n",
      "Train Loss 11500 : 3.6621557647752176\n",
      "Train Loss 12000 : 3.6435862828787045\n",
      "Train Loss 12500 : 3.621811909405158\n",
      "Train Loss 13000 : 3.600437865698487\n",
      "Train Loss 13500 : 3.580238054455779\n",
      "Train Loss 14000 : 3.5582611202963776\n",
      "Epoch 0 - Val Loss: 2.801163695094755, bn_score Val Accuracy: 0.6755585482924135, cl_score Val Accuracy: 0.3150920266583791\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_0.pth\n",
      "Train Loss 0 : 3.5302789211273193\n",
      "Train Loss 500 : 2.890336844497574\n",
      "Train Loss 1000 : 2.858536007163765\n",
      "Train Loss 1500 : 2.8239838259764944\n",
      "Train Loss 2000 : 2.7818635156427485\n",
      "Train Loss 2500 : 2.743887082999442\n",
      "Train Loss 3000 : 2.7094924582993336\n",
      "Train Loss 3500 : 2.676778303048026\n",
      "Train Loss 4000 : 2.642636413277462\n",
      "Train Loss 4500 : 2.608063963380715\n",
      "Train Loss 5000 : 2.575014508216292\n",
      "Train Loss 5500 : 2.5427875188324065\n",
      "Train Loss 6000 : 2.5095210030881034\n",
      "Train Loss 6500 : 2.474655013628289\n",
      "Train Loss 7000 : 2.4413951635360718\n",
      "Train Loss 7500 : 2.414933511033533\n",
      "Train Loss 8000 : 2.3882318341781192\n",
      "Train Loss 8500 : 2.3623467067692814\n",
      "Train Loss 9000 : 2.3383364223093603\n",
      "Train Loss 9500 : 2.3159635607386475\n",
      "Train Loss 10000 : 2.2921432059367364\n",
      "Train Loss 10500 : 2.2689651248057947\n",
      "Train Loss 11000 : 2.2478415124926303\n",
      "Train Loss 11500 : 2.2292073539843424\n",
      "Train Loss 12000 : 2.208314345683786\n",
      "Train Loss 12500 : 2.1911576245462556\n",
      "Train Loss 13000 : 2.1738815276166696\n",
      "Train Loss 13500 : 2.1588966878848854\n",
      "Train Loss 14000 : 2.1426816246554847\n",
      "Epoch 1 - Val Loss: 1.509255505078154, bn_score Val Accuracy: 0.6904848233709727, cl_score Val Accuracy: 0.7350305604475292\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_1.pth\n",
      "Train Loss 0 : 1.9689476490020752\n",
      "Train Loss 500 : 1.621026962460158\n",
      "Train Loss 1000 : 1.6232254784126263\n",
      "Train Loss 1500 : 1.6126751329881361\n",
      "Train Loss 2000 : 1.625401993026619\n",
      "Train Loss 2500 : 1.614314577523254\n",
      "Train Loss 3000 : 1.6114150301212868\n",
      "Train Loss 3500 : 1.6124793310074832\n",
      "Train Loss 4000 : 1.6084393588893564\n",
      "Train Loss 4500 : 1.604584670581968\n",
      "Train Loss 5000 : 1.6039126773007844\n",
      "Train Loss 5500 : 1.5994064433859427\n",
      "Train Loss 6000 : 1.5955509063085502\n",
      "Train Loss 6500 : 1.592319887466677\n",
      "Train Loss 7000 : 1.5853780534902209\n",
      "Train Loss 7500 : 1.5796121303605455\n",
      "Train Loss 8000 : 1.573985368307643\n",
      "Train Loss 8500 : 1.5713848506944375\n",
      "Train Loss 9000 : 1.5654835010976107\n",
      "Train Loss 9500 : 1.5554265460739913\n",
      "Train Loss 10000 : 1.5494360484465184\n",
      "Train Loss 10500 : 1.542522429508909\n",
      "Train Loss 11000 : 1.53856514815406\n",
      "Train Loss 11500 : 1.5336169530936774\n",
      "Train Loss 12000 : 1.5293381097748642\n",
      "Train Loss 12500 : 1.5261526766176006\n",
      "Train Loss 13000 : 1.5219864323064736\n",
      "Train Loss 13500 : 1.5168690275292989\n",
      "Train Loss 14000 : 1.5134208850878135\n",
      "Epoch 2 - Val Loss: 1.174123031889066, bn_score Val Accuracy: 0.7957543423460754, cl_score Val Accuracy: 0.7811129527953313\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_2.pth\n",
      "Train Loss 0 : 0.7808999419212341\n",
      "Train Loss 500 : 1.3318141818403484\n",
      "Train Loss 1000 : 1.3256622815465593\n",
      "Train Loss 1500 : 1.32650568130253\n",
      "Train Loss 2000 : 1.3211935317289942\n",
      "Train Loss 2500 : 1.3035680920970005\n",
      "Train Loss 3000 : 1.300896185382292\n",
      "Train Loss 3500 : 1.3030055925373485\n",
      "Train Loss 4000 : 1.299477854294349\n",
      "Train Loss 4500 : 1.2951504669171443\n",
      "Train Loss 5000 : 1.2938709580595553\n",
      "Train Loss 5500 : 1.2924081361785713\n",
      "Train Loss 6000 : 1.2881430977981223\n",
      "Train Loss 6500 : 1.2878151259189787\n",
      "Train Loss 7000 : 1.283439896393122\n",
      "Train Loss 7500 : 1.2805150475776637\n",
      "Train Loss 8000 : 1.2787471513661157\n",
      "Train Loss 8500 : 1.2751468399572676\n",
      "Train Loss 9000 : 1.2720153153889604\n",
      "Train Loss 9500 : 1.2694466167024006\n",
      "Train Loss 10000 : 1.2660405974026478\n",
      "Train Loss 10500 : 1.2629655832583513\n",
      "Train Loss 11000 : 1.2602961900754144\n",
      "Train Loss 11500 : 1.2568036368834683\n",
      "Train Loss 12000 : 1.2533257349378197\n",
      "Train Loss 12500 : 1.250372489449568\n",
      "Train Loss 13000 : 1.2484028191795606\n",
      "Train Loss 13500 : 1.245551247069963\n",
      "Train Loss 14000 : 1.2422079141257396\n",
      "Epoch 3 - Val Loss: 0.969350731907539, bn_score Val Accuracy: 0.8765150730342899, cl_score Val Accuracy: 0.8017801029041058\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_3.pth\n",
      "Train Loss 0 : 0.7508668899536133\n",
      "Train Loss 500 : 1.1588753954974\n",
      "Train Loss 1000 : 1.1463577946076742\n",
      "Train Loss 1500 : 1.1248800174980302\n",
      "Train Loss 2000 : 1.1234658638174566\n",
      "Train Loss 2500 : 1.1113253436783035\n",
      "Train Loss 3000 : 1.1137448941537118\n",
      "Train Loss 3500 : 1.1080257258255868\n",
      "Train Loss 4000 : 1.109845994066906\n",
      "Train Loss 4500 : 1.109414470921461\n",
      "Train Loss 5000 : 1.105386917316015\n",
      "Train Loss 5500 : 1.1023025574239464\n",
      "Train Loss 6000 : 1.102470190318758\n",
      "Train Loss 6500 : 1.102699662939372\n",
      "Train Loss 7000 : 1.1017814196987827\n",
      "Train Loss 7500 : 1.0976809369232032\n",
      "Train Loss 8000 : 1.096649816821492\n",
      "Train Loss 8500 : 1.0942709591432944\n",
      "Train Loss 9000 : 1.0926909663247515\n",
      "Train Loss 9500 : 1.0916559951474587\n",
      "Train Loss 10000 : 1.091827649850748\n",
      "Train Loss 10500 : 1.0880621090941447\n",
      "Train Loss 11000 : 1.087459245687013\n",
      "Train Loss 11500 : 1.0877832978364295\n",
      "Train Loss 12000 : 1.0855209501426826\n",
      "Train Loss 12500 : 1.0844186908563151\n",
      "Train Loss 13000 : 1.0831427214186247\n",
      "Train Loss 13500 : 1.0808146381585837\n",
      "Train Loss 14000 : 1.0792337562686645\n",
      "Epoch 4 - Val Loss: 0.8550176191074836, bn_score Val Accuracy: 0.9066870402983529, cl_score Val Accuracy: 0.8151179253427259\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_4.pth\n",
      "Train Loss 0 : 1.2168540954589844\n",
      "Train Loss 500 : 0.9770565961232919\n",
      "Train Loss 1000 : 1.0143568848485833\n",
      "Train Loss 1500 : 0.9913607035849806\n",
      "Train Loss 2000 : 0.9943386983603001\n",
      "Train Loss 2500 : 1.0010796635114148\n",
      "Train Loss 3000 : 0.9972552447010332\n",
      "Train Loss 3500 : 0.9931959798248308\n",
      "Train Loss 4000 : 0.9962653795443894\n",
      "Train Loss 4500 : 1.0007943108266517\n",
      "Train Loss 5000 : 1.003772230849409\n",
      "Train Loss 5500 : 1.004024924131385\n",
      "Train Loss 6000 : 1.0035981440246526\n",
      "Train Loss 6500 : 1.0002436831068042\n",
      "Train Loss 7000 : 0.9994768314662171\n",
      "Train Loss 7500 : 0.9999247423948395\n",
      "Train Loss 8000 : 0.9977049352057579\n",
      "Train Loss 8500 : 0.9988071105981182\n",
      "Train Loss 9000 : 0.9991877711657507\n",
      "Train Loss 9500 : 1.000342629606616\n",
      "Train Loss 10000 : 0.9986599392627609\n",
      "Train Loss 10500 : 0.998240003131352\n",
      "Train Loss 11000 : 0.9962142382068367\n",
      "Train Loss 11500 : 0.9926550033698327\n",
      "Train Loss 12000 : 0.9926481495775347\n",
      "Train Loss 12500 : 0.9921760968841614\n",
      "Train Loss 13000 : 0.9899331856351072\n",
      "Train Loss 13500 : 0.9913663760545852\n",
      "Train Loss 14000 : 0.9895841404304742\n",
      "Epoch 5 - Val Loss: 0.7761801667695418, bn_score Val Accuracy: 0.9213888601125729, cl_score Val Accuracy: 0.8230170240685106\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_5.pth\n",
      "Train Loss 0 : 0.2630084753036499\n",
      "Train Loss 500 : 0.9027857618358083\n",
      "Train Loss 1000 : 0.8969547223452327\n",
      "Train Loss 1500 : 0.9018780802048341\n",
      "Train Loss 2000 : 0.9046279500643055\n",
      "Train Loss 2500 : 0.9170387691137839\n",
      "Train Loss 3000 : 0.9139006483228613\n",
      "Train Loss 3500 : 0.9117205832818599\n",
      "Train Loss 4000 : 0.9169729527833684\n",
      "Train Loss 4500 : 0.9170369441813229\n",
      "Train Loss 5000 : 0.9158486155124514\n",
      "Train Loss 5500 : 0.9149244350777866\n",
      "Train Loss 6000 : 0.9127186522250313\n",
      "Train Loss 6500 : 0.913587979123127\n",
      "Train Loss 7000 : 0.9146493832371759\n",
      "Train Loss 7500 : 0.9149567123878449\n",
      "Train Loss 8000 : 0.9178356071055278\n",
      "Train Loss 8500 : 0.9169932909970696\n",
      "Train Loss 9000 : 0.9177773550546787\n",
      "Train Loss 9500 : 0.9175891577487788\n",
      "Train Loss 10000 : 0.9187502286830594\n",
      "Train Loss 10500 : 0.9182597130888951\n",
      "Train Loss 11000 : 0.9214150139488163\n",
      "Train Loss 11500 : 0.9207569774120085\n",
      "Train Loss 12000 : 0.9216314782506078\n",
      "Train Loss 12500 : 0.9221160969033965\n",
      "Train Loss 13000 : 0.9234421579147906\n",
      "Train Loss 13500 : 0.9243007934598532\n",
      "Train Loss 14000 : 0.9240109048247376\n",
      "Epoch 6 - Val Loss: 0.7352887840976426, bn_score Val Accuracy: 0.9272937601436514, cl_score Val Accuracy: 0.8315376912186194\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_6.pth\n",
      "Train Loss 0 : 0.7603871822357178\n",
      "Train Loss 500 : 0.892724841773629\n",
      "Train Loss 1000 : 0.8860698427547227\n",
      "Train Loss 1500 : 0.8789180728304712\n",
      "Train Loss 2000 : 0.8689124007420294\n",
      "Train Loss 2500 : 0.8666556148228646\n",
      "Train Loss 3000 : 0.8650480471799689\n",
      "Train Loss 3500 : 0.8655934940477723\n",
      "Train Loss 4000 : 0.8628470705339341\n",
      "Train Loss 4500 : 0.8663730796710422\n",
      "Train Loss 5000 : 0.8637577464449849\n",
      "Train Loss 5500 : 0.8675567278558234\n",
      "Train Loss 6000 : 0.8711869814379616\n",
      "Train Loss 6500 : 0.8696259993792195\n",
      "Train Loss 7000 : 0.8716121710264338\n",
      "Train Loss 7500 : 0.8716171806870127\n",
      "Train Loss 8000 : 0.8709484886863674\n",
      "Train Loss 8500 : 0.8733510068204631\n",
      "Train Loss 9000 : 0.8727118215367319\n",
      "Train Loss 9500 : 0.8712469650925134\n",
      "Train Loss 10000 : 0.8715012790224556\n",
      "Train Loss 10500 : 0.8700298829950125\n",
      "Train Loss 11000 : 0.8700425380229836\n",
      "Train Loss 11500 : 0.8710761487826675\n",
      "Train Loss 12000 : 0.8719559277758361\n",
      "Train Loss 12500 : 0.8729611082776872\n",
      "Train Loss 13000 : 0.8724677959340562\n",
      "Train Loss 13500 : 0.872593467907646\n",
      "Train Loss 14000 : 0.8726884631167185\n",
      "Epoch 7 - Val Loss: 0.701214739817054, bn_score Val Accuracy: 0.9321281812217272, cl_score Val Accuracy: 0.8364066438758244\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_7.pth\n",
      "Train Loss 0 : 1.0968964099884033\n",
      "Train Loss 500 : 0.8091971060324333\n",
      "Train Loss 1000 : 0.8154775217197813\n",
      "Train Loss 1500 : 0.8105295834805988\n",
      "Train Loss 2000 : 0.8181076213225968\n",
      "Train Loss 2500 : 0.8236956535185017\n",
      "Train Loss 3000 : 0.825608939270066\n",
      "Train Loss 3500 : 0.8284845660900566\n",
      "Train Loss 4000 : 0.832388685285762\n",
      "Train Loss 4500 : 0.8324817086209313\n",
      "Train Loss 5000 : 0.8331487819662668\n",
      "Train Loss 5500 : 0.834690286417134\n",
      "Train Loss 6000 : 0.8319465453325063\n",
      "Train Loss 6500 : 0.8342089175225597\n",
      "Train Loss 7000 : 0.8307614590110114\n",
      "Train Loss 7500 : 0.830649339969282\n",
      "Train Loss 8000 : 0.82852737975959\n",
      "Train Loss 8500 : 0.8278114652391946\n",
      "Train Loss 9000 : 0.8284498760711583\n",
      "Train Loss 9500 : 0.8272775067899676\n",
      "Train Loss 10000 : 0.8263575064345975\n",
      "Train Loss 10500 : 0.8293545048213121\n",
      "Train Loss 11000 : 0.8285322916008171\n",
      "Train Loss 11500 : 0.8288760874558067\n",
      "Train Loss 12000 : 0.8293178307443345\n",
      "Train Loss 12500 : 0.8313231690357734\n",
      "Train Loss 13000 : 0.8323080707009047\n",
      "Train Loss 13500 : 0.8329798575768296\n",
      "Train Loss 14000 : 0.8323282996868001\n",
      "Epoch 8 - Val Loss: 0.6753541564413641, bn_score Val Accuracy: 0.9338806588625298, cl_score Val Accuracy: 0.8424151386442902\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_8.pth\n",
      "Train Loss 0 : 0.3296066224575043\n",
      "Train Loss 500 : 0.7948916247281843\n",
      "Train Loss 1000 : 0.789601291061847\n",
      "Train Loss 1500 : 0.783023367172416\n",
      "Train Loss 2000 : 0.7807002413602754\n",
      "Train Loss 2500 : 0.7775652910925683\n",
      "Train Loss 3000 : 0.7828577043229801\n",
      "Train Loss 3500 : 0.7843952387639435\n",
      "Train Loss 4000 : 0.7843537186755445\n",
      "Train Loss 4500 : 0.7840349003151175\n",
      "Train Loss 5000 : 0.7848039620791142\n",
      "Train Loss 5500 : 0.7853199719916775\n",
      "Train Loss 6000 : 0.7892090153049041\n",
      "Train Loss 6500 : 0.7893711163220708\n",
      "Train Loss 7000 : 0.7886236809112144\n",
      "Train Loss 7500 : 0.7931345012513237\n",
      "Train Loss 8000 : 0.7945796477021374\n",
      "Train Loss 8500 : 0.7969226587185572\n",
      "Train Loss 9000 : 0.7987763003394738\n",
      "Train Loss 9500 : 0.7999310896413042\n",
      "Train Loss 10000 : 0.8002623721242768\n",
      "Train Loss 10500 : 0.7995467299557162\n",
      "Train Loss 11000 : 0.8007668475293246\n",
      "Train Loss 11500 : 0.7996718118473519\n",
      "Train Loss 12000 : 0.8001539052241577\n",
      "Train Loss 12500 : 0.7994009667979779\n",
      "Train Loss 13000 : 0.7995355189471672\n",
      "Train Loss 13500 : 0.8011195529410259\n",
      "Train Loss 14000 : 0.8008002549737535\n",
      "Epoch 9 - Val Loss: 0.6356859872110069, bn_score Val Accuracy: 0.9414603404813702, cl_score Val Accuracy: 0.8467488518249939\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_9.pth\n",
      "Train Loss 0 : 0.4873487949371338\n",
      "Train Loss 500 : 0.7675684971545271\n",
      "Train Loss 1000 : 0.7630090992079004\n",
      "Train Loss 1500 : 0.766633730910366\n",
      "Train Loss 2000 : 0.7749321167261853\n",
      "Train Loss 2500 : 0.7684444307279987\n",
      "Train Loss 3000 : 0.7693763322953193\n",
      "Train Loss 3500 : 0.7699502576690542\n",
      "Train Loss 4000 : 0.7669918074704831\n",
      "Train Loss 4500 : 0.7643267198121666\n",
      "Train Loss 5000 : 0.7634212025209376\n",
      "Train Loss 5500 : 0.7646301247933235\n",
      "Train Loss 6000 : 0.765643250014353\n",
      "Train Loss 6500 : 0.7694997572292939\n",
      "Train Loss 7000 : 0.769734933713488\n",
      "Train Loss 7500 : 0.770879868442562\n",
      "Train Loss 8000 : 0.7709272751721303\n",
      "Train Loss 8500 : 0.7713085421988886\n",
      "Train Loss 9000 : 0.7708405773470454\n",
      "Train Loss 9500 : 0.7709319517912346\n",
      "Train Loss 10000 : 0.7732659199009396\n",
      "Train Loss 10500 : 0.7739565246427639\n",
      "Train Loss 11000 : 0.7738185957243142\n",
      "Train Loss 11500 : 0.7770889536498274\n",
      "Train Loss 12000 : 0.777672669679484\n",
      "Train Loss 12500 : 0.7765333944021685\n",
      "Train Loss 13000 : 0.778512314431763\n",
      "Train Loss 13500 : 0.7782519020811501\n",
      "Train Loss 14000 : 0.7779743277635613\n",
      "Epoch 10 - Val Loss: 0.6282895354350018, bn_score Val Accuracy: 0.9400790773162057, cl_score Val Accuracy: 0.8495113781553231\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_10.pth\n",
      "Train Loss 0 : 0.6081050038337708\n",
      "Train Loss 500 : 0.7025027717956288\n",
      "Train Loss 1000 : 0.7026656070123096\n",
      "Train Loss 1500 : 0.705062347773877\n",
      "Train Loss 2000 : 0.7005190326004237\n",
      "Train Loss 2500 : 0.7076008917457732\n",
      "Train Loss 3000 : 0.7102096975672095\n",
      "Train Loss 3500 : 0.7141184059278419\n",
      "Train Loss 4000 : 0.7155892708846474\n",
      "Train Loss 4500 : 0.7205541997581433\n",
      "Train Loss 5000 : 0.7233969287636193\n",
      "Train Loss 5500 : 0.7275645783336295\n",
      "Train Loss 6000 : 0.7294823592219863\n",
      "Train Loss 6500 : 0.733679816830795\n",
      "Train Loss 7000 : 0.7367687095946358\n",
      "Train Loss 7500 : 0.7375107583675646\n",
      "Train Loss 8000 : 0.7414697803798567\n",
      "Train Loss 8500 : 0.7416919344262037\n",
      "Train Loss 9000 : 0.7441122559297683\n",
      "Train Loss 9500 : 0.7468780425553927\n",
      "Train Loss 10000 : 0.7479414292379771\n",
      "Train Loss 10500 : 0.7480084467053119\n",
      "Train Loss 11000 : 0.7479947770481548\n",
      "Train Loss 11500 : 0.7481722375699493\n",
      "Train Loss 12000 : 0.7478247090147273\n",
      "Train Loss 12500 : 0.7488537020850192\n",
      "Train Loss 13000 : 0.7500237377724224\n",
      "Train Loss 13500 : 0.7516385563668208\n",
      "Train Loss 14000 : 0.752889049205865\n",
      "Epoch 11 - Val Loss: 0.6031156524328667, bn_score Val Accuracy: 0.9456990918194689, cl_score Val Accuracy: 0.8513156531648193\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_11.pth\n",
      "Train Loss 0 : 0.6289782524108887\n",
      "Train Loss 500 : 0.6785598099603506\n",
      "Train Loss 1000 : 0.6762067382107605\n",
      "Train Loss 1500 : 0.6784975000704917\n",
      "Train Loss 2000 : 0.6884325395069685\n",
      "Train Loss 2500 : 0.6963017334698326\n",
      "Train Loss 3000 : 0.6949839673403208\n",
      "Train Loss 3500 : 0.7035645452138889\n",
      "Train Loss 4000 : 0.7045602726314093\n",
      "Train Loss 4500 : 0.7054325535066196\n",
      "Train Loss 5000 : 0.7075248733339796\n",
      "Train Loss 5500 : 0.7059400116532453\n",
      "Train Loss 6000 : 0.7054841496254284\n",
      "Train Loss 6500 : 0.7053841319583544\n",
      "Train Loss 7000 : 0.7077104318152686\n",
      "Train Loss 7500 : 0.7100545006597194\n",
      "Train Loss 8000 : 0.7133041209222439\n",
      "Train Loss 8500 : 0.7173654064731102\n",
      "Train Loss 9000 : 0.7170069673526054\n",
      "Train Loss 9500 : 0.7186996546822819\n",
      "Train Loss 10000 : 0.7199529008497528\n",
      "Train Loss 10500 : 0.7216261403702371\n",
      "Train Loss 11000 : 0.7226327066776639\n",
      "Train Loss 11500 : 0.7244707452227718\n",
      "Train Loss 12000 : 0.7257696370741729\n",
      "Train Loss 12500 : 0.7278520688369874\n",
      "Train Loss 13000 : 0.7267174450934917\n",
      "Train Loss 13500 : 0.7277890447186124\n",
      "Train Loss 14000 : 0.7290134348724782\n",
      "Epoch 12 - Val Loss: 0.5913208339264823, bn_score Val Accuracy: 0.9485393142028385, cl_score Val Accuracy: 0.8546738492351256\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_12.pth\n",
      "Train Loss 0 : 0.8599210977554321\n",
      "Train Loss 500 : 0.6809438411927152\n",
      "Train Loss 1000 : 0.6958716030415896\n",
      "Train Loss 1500 : 0.697686692486349\n",
      "Train Loss 2000 : 0.7039603030226235\n",
      "Train Loss 2500 : 0.6986150504851409\n",
      "Train Loss 3000 : 0.6994819639409529\n",
      "Train Loss 3500 : 0.6967309497494194\n",
      "Train Loss 4000 : 0.6988455437107947\n",
      "Train Loss 4500 : 0.6989607314192879\n",
      "Train Loss 5000 : 0.6963785332555897\n",
      "Train Loss 5500 : 0.6946963387734645\n",
      "Train Loss 6000 : 0.6950343501547851\n",
      "Train Loss 6500 : 0.6955971135522203\n",
      "Train Loss 7000 : 0.700524715519537\n",
      "Train Loss 7500 : 0.7009285761236126\n",
      "Train Loss 8000 : 0.7030281608015413\n",
      "Train Loss 8500 : 0.7052532408270122\n",
      "Train Loss 9000 : 0.7062964932221177\n",
      "Train Loss 9500 : 0.706039807048554\n",
      "Train Loss 10000 : 0.7073830485027501\n",
      "Train Loss 10500 : 0.7084145448186123\n",
      "Train Loss 11000 : 0.7081572051165033\n",
      "Train Loss 11500 : 0.7102627166955089\n",
      "Train Loss 12000 : 0.7093861213692092\n",
      "Train Loss 12500 : 0.7106435016241187\n",
      "Train Loss 13000 : 0.7126922442745905\n",
      "Train Loss 13500 : 0.7130224194899025\n",
      "Train Loss 14000 : 0.713149887291198\n",
      "Epoch 13 - Val Loss: 0.5723933553759851, bn_score Val Accuracy: 0.9487637694671778, cl_score Val Accuracy: 0.8572809834593736\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_13.pth\n",
      "Train Loss 0 : 0.49467626214027405\n",
      "Train Loss 500 : 0.645516568884075\n",
      "Train Loss 1000 : 0.6556008993764292\n",
      "Train Loss 1500 : 0.6716350207389453\n",
      "Train Loss 2000 : 0.6791455715621028\n",
      "Train Loss 2500 : 0.6806717966711138\n",
      "Train Loss 3000 : 0.6848879284208336\n",
      "Train Loss 3500 : 0.6845655782167928\n",
      "Train Loss 4000 : 0.6820917494717955\n",
      "Train Loss 4500 : 0.682037719344657\n",
      "Train Loss 5000 : 0.6817889496302495\n",
      "Train Loss 5500 : 0.6829863780808266\n",
      "Train Loss 6000 : 0.6830571544453796\n",
      "Train Loss 6500 : 0.6839372785070731\n",
      "Train Loss 7000 : 0.6855959152515353\n",
      "Train Loss 7500 : 0.6882069003065521\n",
      "Train Loss 8000 : 0.6898406589111699\n",
      "Train Loss 8500 : 0.6923595021136802\n",
      "Train Loss 9000 : 0.6961017985997496\n",
      "Train Loss 9500 : 0.6953745550446019\n",
      "Train Loss 10000 : 0.6936495890553603\n",
      "Train Loss 10500 : 0.6944008985733949\n",
      "Train Loss 11000 : 0.6942913176792138\n",
      "Train Loss 11500 : 0.6944440384079609\n",
      "Train Loss 12000 : 0.6953808707535859\n",
      "Train Loss 12500 : 0.6953883096444989\n",
      "Train Loss 13000 : 0.6961066251254668\n",
      "Train Loss 13500 : 0.6960410377501922\n",
      "Train Loss 14000 : 0.6980854929873821\n",
      "Epoch 14 - Val Loss: 0.55914881124954, bn_score Val Accuracy: 0.9504903484236334, cl_score Val Accuracy: 0.8603888255809938\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_14.pth\n",
      "Train Loss 0 : 1.0981628894805908\n",
      "Train Loss 500 : 0.6477902621670278\n",
      "Train Loss 1000 : 0.6369813580817723\n",
      "Train Loss 1500 : 0.6382019767806996\n",
      "Train Loss 2000 : 0.6504886443161223\n",
      "Train Loss 2500 : 0.6618541002650408\n",
      "Train Loss 3000 : 0.6681754325672622\n",
      "Train Loss 3500 : 0.6742755951074166\n",
      "Train Loss 4000 : 0.6747615682582682\n",
      "Train Loss 4500 : 0.674007816137148\n",
      "Train Loss 5000 : 0.6757836840012131\n",
      "Train Loss 5500 : 0.6766955669687604\n",
      "Train Loss 6000 : 0.6793904702488437\n",
      "Train Loss 6500 : 0.6809197847098639\n",
      "Train Loss 7000 : 0.6812020206163865\n",
      "Train Loss 7500 : 0.6819171264867968\n",
      "Train Loss 8000 : 0.6848535425567583\n",
      "Train Loss 8500 : 0.6847549761370405\n",
      "Train Loss 9000 : 0.6854471148713619\n",
      "Train Loss 9500 : 0.6846234705368736\n",
      "Train Loss 10000 : 0.6847543675986437\n",
      "Train Loss 10500 : 0.6835555867643639\n",
      "Train Loss 11000 : 0.6844433392675314\n",
      "Train Loss 11500 : 0.6861515828745897\n",
      "Train Loss 12000 : 0.6879207937345815\n",
      "Train Loss 12500 : 0.6881353142974874\n",
      "Train Loss 13000 : 0.6893940214842068\n",
      "Train Loss 13500 : 0.6895810816073197\n",
      "Train Loss 14000 : 0.6897829164991737\n",
      "Epoch 15 - Val Loss: 0.5592173093200352, bn_score Val Accuracy: 0.9521651300113955, cl_score Val Accuracy: 0.8597154597879761\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_15.pth\n",
      "Train Loss 0 : 0.37474435567855835\n",
      "Train Loss 500 : 0.649111797449415\n",
      "Train Loss 1000 : 0.6442809343248695\n",
      "Train Loss 1500 : 0.656440324836884\n",
      "Train Loss 2000 : 0.6546965985531467\n",
      "Train Loss 2500 : 0.6470699252874279\n",
      "Train Loss 3000 : 0.6484641808000422\n",
      "Train Loss 3500 : 0.6589489724727545\n",
      "Train Loss 4000 : 0.6575933265313786\n",
      "Train Loss 4500 : 0.6566094669271935\n",
      "Train Loss 5000 : 0.6573033826051354\n",
      "Train Loss 5500 : 0.6585770117339385\n",
      "Train Loss 6000 : 0.6582071233725144\n",
      "Train Loss 6500 : 0.6583333202078882\n",
      "Train Loss 7000 : 0.660107102307083\n",
      "Train Loss 7500 : 0.6620192663703616\n",
      "Train Loss 8000 : 0.6626893695129121\n",
      "Train Loss 8500 : 0.6627837633296677\n",
      "Train Loss 9000 : 0.6652938271179976\n",
      "Train Loss 9500 : 0.6654111660280587\n",
      "Train Loss 10000 : 0.6673989497466352\n",
      "Train Loss 10500 : 0.6702084954345201\n",
      "Train Loss 11000 : 0.6702398843650151\n",
      "Train Loss 11500 : 0.6732345398911604\n",
      "Train Loss 12000 : 0.6745234640151742\n",
      "Train Loss 12500 : 0.6751778358729917\n",
      "Train Loss 13000 : 0.6751736334567529\n",
      "Train Loss 13500 : 0.6760976778971192\n",
      "Train Loss 14000 : 0.6764187173009649\n",
      "Epoch 16 - Val Loss: 0.5413249134265071, bn_score Val Accuracy: 0.9548413273939017, cl_score Val Accuracy: 0.8640146413895508\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_16.pth\n",
      "Train Loss 0 : 0.639109194278717\n",
      "Train Loss 500 : 0.6200577923935092\n",
      "Train Loss 1000 : 0.6320358921739307\n",
      "Train Loss 1500 : 0.6299482442681826\n",
      "Train Loss 2000 : 0.6251198727672902\n",
      "Train Loss 2500 : 0.6296818155224849\n",
      "Train Loss 3000 : 0.6338127656488797\n",
      "Train Loss 3500 : 0.6337116826272452\n",
      "Train Loss 4000 : 0.6360734057126456\n",
      "Train Loss 4500 : 0.6394634257411922\n",
      "Train Loss 5000 : 0.6412156548587931\n",
      "Train Loss 5500 : 0.6398868341123494\n",
      "Train Loss 6000 : 0.641647778438872\n",
      "Train Loss 6500 : 0.6433039569165434\n",
      "Train Loss 7000 : 0.6456099666356817\n",
      "Train Loss 7500 : 0.6478903533569518\n",
      "Train Loss 8000 : 0.6505611890539575\n",
      "Train Loss 8500 : 0.6515780528981022\n",
      "Train Loss 9000 : 0.6541361847022024\n",
      "Train Loss 9500 : 0.6547547178073373\n",
      "Train Loss 10000 : 0.655612779285622\n",
      "Train Loss 10500 : 0.6577191192696843\n",
      "Train Loss 11000 : 0.6574169652448975\n",
      "Train Loss 11500 : 0.6596273212401221\n",
      "Train Loss 12000 : 0.6615355067546741\n",
      "Train Loss 12500 : 0.6627367874497929\n",
      "Train Loss 13000 : 0.6631472552313102\n",
      "Train Loss 13500 : 0.6645675292763544\n",
      "Train Loss 14000 : 0.6654060756877523\n",
      "Epoch 17 - Val Loss: 0.5346022814763398, bn_score Val Accuracy: 0.9554801616077904, cl_score Val Accuracy: 0.8654131703442799\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_17.pth\n",
      "Train Loss 0 : 0.7701476812362671\n",
      "Train Loss 500 : 0.6072659058842117\n",
      "Train Loss 1000 : 0.6275736250534103\n",
      "Train Loss 1500 : 0.6306041898050759\n",
      "Train Loss 2000 : 0.6264700478446418\n",
      "Train Loss 2500 : 0.6288002995682545\n",
      "Train Loss 3000 : 0.6266940694764103\n",
      "Train Loss 3500 : 0.6249650256312964\n",
      "Train Loss 4000 : 0.6319298417384485\n",
      "Train Loss 4500 : 0.6329235633219833\n",
      "Train Loss 5000 : 0.6358773149263172\n",
      "Train Loss 5500 : 0.6350499249822407\n",
      "Train Loss 6000 : 0.6375185598171856\n",
      "Train Loss 6500 : 0.63979590985104\n",
      "Train Loss 7000 : 0.6399384396986917\n",
      "Train Loss 7500 : 0.6391120990610388\n",
      "Train Loss 8000 : 0.6415390821266675\n",
      "Train Loss 8500 : 0.6442555784391442\n",
      "Train Loss 9000 : 0.642203589568676\n",
      "Train Loss 9500 : 0.6436166324484229\n",
      "Train Loss 10000 : 0.6440173102063733\n",
      "Train Loss 10500 : 0.6455552673666893\n",
      "Train Loss 11000 : 0.6491225045307273\n",
      "Train Loss 11500 : 0.6511463339386027\n",
      "Train Loss 12000 : 0.6525829876975994\n",
      "Train Loss 12500 : 0.653198172773535\n",
      "Train Loss 13000 : 0.6554315772285656\n",
      "Train Loss 13500 : 0.6553716633265922\n",
      "Train Loss 14000 : 0.6567580328509797\n",
      "Epoch 18 - Val Loss: 0.5244869374703924, bn_score Val Accuracy: 0.9553247695017093, cl_score Val Accuracy: 0.8671311164059532\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_18.pth\n",
      "Train Loss 0 : 0.9633350372314453\n",
      "Train Loss 500 : 0.641351051386929\n",
      "Train Loss 1000 : 0.6394103853623723\n",
      "Train Loss 1500 : 0.6298592482543961\n",
      "Train Loss 2000 : 0.6230346122891575\n",
      "Train Loss 2500 : 0.6283191166853891\n",
      "Train Loss 3000 : 0.6268691904482863\n",
      "Train Loss 3500 : 0.6289881220440147\n",
      "Train Loss 4000 : 0.6260625671612094\n",
      "Train Loss 4500 : 0.6253754333364311\n",
      "Train Loss 5000 : 0.6229060913151746\n",
      "Train Loss 5500 : 0.6259382489663707\n",
      "Train Loss 6000 : 0.6310279244385378\n",
      "Train Loss 6500 : 0.62844928074843\n",
      "Train Loss 7000 : 0.6303400375856755\n",
      "Train Loss 7500 : 0.6304962341903989\n",
      "Train Loss 8000 : 0.6304470103261057\n",
      "Train Loss 8500 : 0.632243815919481\n",
      "Train Loss 9000 : 0.6336003413200196\n",
      "Train Loss 9500 : 0.6348994900319055\n",
      "Train Loss 10000 : 0.6373211057271309\n",
      "Train Loss 10500 : 0.6380898710769516\n",
      "Train Loss 11000 : 0.6396072930565366\n",
      "Train Loss 11500 : 0.6399686264359876\n",
      "Train Loss 12000 : 0.6417638578176443\n",
      "Train Loss 12500 : 0.6441202990140961\n",
      "Train Loss 13000 : 0.6458266188294061\n",
      "Train Loss 13500 : 0.6461483460974697\n",
      "Train Loss 14000 : 0.6468111133052512\n",
      "Epoch 19 - Val Loss: 0.5167219383752016, bn_score Val Accuracy: 0.9573017024068511, cl_score Val Accuracy: 0.867372837459857\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_19.pth\n",
      "Train Loss 0 : 0.5154781341552734\n",
      "Train Loss 500 : 0.5922682050563499\n",
      "Train Loss 1000 : 0.6222920369126134\n",
      "Train Loss 1500 : 0.6015732527861272\n",
      "Train Loss 2000 : 0.6033073295985741\n",
      "Train Loss 2500 : 0.6056017176669295\n",
      "Train Loss 3000 : 0.6114439979395452\n",
      "Train Loss 3500 : 0.6123225186768971\n",
      "Train Loss 4000 : 0.6169047046384558\n",
      "Train Loss 4500 : 0.6203005692712181\n",
      "Train Loss 5000 : 0.6209041863500921\n",
      "Train Loss 5500 : 0.6215447423361862\n",
      "Train Loss 6000 : 0.6239793326183775\n",
      "Train Loss 6500 : 0.6235830920132115\n",
      "Train Loss 7000 : 0.6248264991214831\n",
      "Train Loss 7500 : 0.6281039979393397\n",
      "Train Loss 8000 : 0.6291217897758387\n",
      "Train Loss 8500 : 0.6322638697402613\n",
      "Train Loss 9000 : 0.6312834868693774\n",
      "Train Loss 9500 : 0.6318153579698388\n",
      "Train Loss 10000 : 0.6319407739574286\n",
      "Train Loss 10500 : 0.6330298274322587\n",
      "Train Loss 11000 : 0.6339354519584145\n",
      "Train Loss 11500 : 0.6349283893819878\n",
      "Train Loss 12000 : 0.6367884306592915\n",
      "Train Loss 12500 : 0.6376013747260285\n",
      "Train Loss 13000 : 0.6390354268959669\n",
      "Train Loss 13500 : 0.6385074688671668\n",
      "Train Loss 14000 : 0.6402989133979807\n",
      "Epoch 20 - Val Loss: 0.5112172853467981, bn_score Val Accuracy: 0.9593649642598157, cl_score Val Accuracy: 0.8709554887945026\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_20.pth\n",
      "Train Loss 0 : 0.656417191028595\n",
      "Train Loss 500 : 0.5798518220297323\n",
      "Train Loss 1000 : 0.5966582142735843\n",
      "Train Loss 1500 : 0.5849374110477268\n",
      "Train Loss 2000 : 0.5888687221231921\n",
      "Train Loss 2500 : 0.5958750136688155\n",
      "Train Loss 3000 : 0.5962935386437231\n",
      "Train Loss 3500 : 0.6005723150295382\n",
      "Train Loss 4000 : 0.608071057685511\n",
      "Train Loss 4500 : 0.6073455774907905\n",
      "Train Loss 5000 : 0.6110196253566015\n",
      "Train Loss 5500 : 0.61095974523824\n",
      "Train Loss 6000 : 0.6129048782649253\n",
      "Train Loss 6500 : 0.6164397341396199\n",
      "Train Loss 7000 : 0.6144249601237217\n",
      "Train Loss 7500 : 0.6170324221116322\n",
      "Train Loss 8000 : 0.6173734043695968\n",
      "Train Loss 8500 : 0.619784264063607\n",
      "Train Loss 9000 : 0.6209177064866431\n",
      "Train Loss 9500 : 0.6217553033382692\n",
      "Train Loss 10000 : 0.6259764377776924\n",
      "Train Loss 10500 : 0.6269182530575\n",
      "Train Loss 11000 : 0.6255138416187214\n",
      "Train Loss 11500 : 0.6268135830942606\n",
      "Train Loss 12000 : 0.6261444534910782\n",
      "Train Loss 12500 : 0.6275695234378217\n",
      "Train Loss 13000 : 0.6274614289428965\n",
      "Train Loss 13500 : 0.6291465022742324\n",
      "Train Loss 14000 : 0.6297257015910036\n",
      "Epoch 21 - Val Loss: 0.51258202952958, bn_score Val Accuracy: 0.9593908629441624, cl_score Val Accuracy: 0.8679080769363583\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_21.pth\n",
      "Train Loss 0 : 1.0781219005584717\n",
      "Train Loss 500 : 0.6136070932047869\n",
      "Train Loss 1000 : 0.6093533242644219\n",
      "Train Loss 1500 : 0.6082172223232155\n",
      "Train Loss 2000 : 0.6058027262835279\n",
      "Train Loss 2500 : 0.6115052935297562\n",
      "Train Loss 3000 : 0.6132774149495656\n",
      "Train Loss 3500 : 0.6043526664066616\n",
      "Train Loss 4000 : 0.611261439535062\n",
      "Train Loss 4500 : 0.6117568181637717\n",
      "Train Loss 5000 : 0.6080347279828267\n",
      "Train Loss 5500 : 0.6086775835354943\n",
      "Train Loss 6000 : 0.6084635921289445\n",
      "Train Loss 6500 : 0.608053393041309\n",
      "Train Loss 7000 : 0.6081843077414772\n",
      "Train Loss 7500 : 0.6083332502647938\n",
      "Train Loss 8000 : 0.6076376869466257\n",
      "Train Loss 8500 : 0.6098194890309933\n",
      "Train Loss 9000 : 0.6116266506026162\n",
      "Train Loss 9500 : 0.6141991515510289\n",
      "Train Loss 10000 : 0.6164558518152923\n",
      "Train Loss 10500 : 0.6164210614631318\n",
      "Train Loss 11000 : 0.6179045280377302\n",
      "Train Loss 11500 : 0.6186160755677119\n",
      "Train Loss 12000 : 0.618391275707064\n",
      "Train Loss 12500 : 0.6197943517639661\n",
      "Train Loss 13000 : 0.6191189934330289\n",
      "Train Loss 13500 : 0.6208160416169372\n",
      "Train Loss 14000 : 0.6216379989947121\n",
      "Epoch 22 - Val Loss: 0.5002924621157109, bn_score Val Accuracy: 0.9595462550502435, cl_score Val Accuracy: 0.8710504506371076\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_22.pth\n",
      "Train Loss 0 : 0.35161763429641724\n",
      "Train Loss 500 : 0.6009769802578463\n",
      "Train Loss 1000 : 0.5912378534058725\n",
      "Train Loss 1500 : 0.5858881153119139\n",
      "Train Loss 2000 : 0.5870070654176537\n",
      "Train Loss 2500 : 0.5906164160982126\n",
      "Train Loss 3000 : 0.5944577939383207\n",
      "Train Loss 3500 : 0.5905192939245898\n",
      "Train Loss 4000 : 0.5969286808291098\n",
      "Train Loss 4500 : 0.5996060405691915\n",
      "Train Loss 5000 : 0.6024284485590299\n",
      "Train Loss 5500 : 0.6060589286148274\n",
      "Train Loss 6000 : 0.6055214413524902\n",
      "Train Loss 6500 : 0.6067390668684687\n",
      "Train Loss 7000 : 0.6082306699559263\n",
      "Train Loss 7500 : 0.6080690210967846\n",
      "Train Loss 8000 : 0.6101783449912193\n",
      "Train Loss 8500 : 0.6099324418047684\n",
      "Train Loss 9000 : 0.6122162243834643\n",
      "Train Loss 9500 : 0.611262269377163\n",
      "Train Loss 10000 : 0.6129686914179215\n",
      "Train Loss 10500 : 0.6160031313749499\n",
      "Train Loss 11000 : 0.616363837525024\n",
      "Train Loss 11500 : 0.6168330380013758\n",
      "Train Loss 12000 : 0.6190975736221737\n",
      "Train Loss 12500 : 0.6199860684766233\n",
      "Train Loss 13000 : 0.6213586599461169\n",
      "Train Loss 13500 : 0.6224595212960365\n",
      "Train Loss 14000 : 0.6204411183170199\n",
      "Epoch 23 - Val Loss: 0.4963772221656473, bn_score Val Accuracy: 0.9615404537449498, cl_score Val Accuracy: 0.8715856901136089\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_23.pth\n",
      "Train Loss 0 : 0.3139198422431946\n",
      "Train Loss 500 : 0.569826984001015\n",
      "Train Loss 1000 : 0.5843343478874057\n",
      "Train Loss 1500 : 0.5863580963699064\n",
      "Train Loss 2000 : 0.5939426576376132\n",
      "Train Loss 2500 : 0.593395740359609\n",
      "Train Loss 3000 : 0.5886571289912818\n",
      "Train Loss 3500 : 0.5938043823797959\n",
      "Train Loss 4000 : 0.5981331549130261\n",
      "Train Loss 4500 : 0.5966163796190213\n",
      "Train Loss 5000 : 0.591995115246746\n",
      "Train Loss 5500 : 0.592952834359216\n",
      "Train Loss 6000 : 0.5929189139861167\n",
      "Train Loss 6500 : 0.594803696962217\n",
      "Train Loss 7000 : 0.5952735801124693\n",
      "Train Loss 7500 : 0.5960927671983063\n",
      "Train Loss 8000 : 0.5976074050422029\n",
      "Train Loss 8500 : 0.5985870322540912\n",
      "Train Loss 9000 : 0.5991872514860055\n",
      "Train Loss 9500 : 0.6023842927496149\n",
      "Train Loss 10000 : 0.6035265826292182\n",
      "Train Loss 10500 : 0.6067807477934903\n",
      "Train Loss 11000 : 0.6061766611532128\n",
      "Train Loss 11500 : 0.6091595225631086\n",
      "Train Loss 12000 : 0.608592743439917\n",
      "Train Loss 12500 : 0.6098720033688989\n",
      "Train Loss 13000 : 0.6120892529052492\n",
      "Train Loss 13500 : 0.6143448829381022\n",
      "Train Loss 14000 : 0.6146482492529474\n",
      "Epoch 24 - Val Loss: 0.4947742405112396, bn_score Val Accuracy: 0.961454124797127, cl_score Val Accuracy: 0.8727424980144342\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_24.pth\n",
      "Train Loss 0 : 1.1110806465148926\n",
      "Train Loss 500 : 0.5908900131573697\n",
      "Train Loss 1000 : 0.5893909882207017\n",
      "Train Loss 1500 : 0.581774126678745\n",
      "Train Loss 2000 : 0.5796560360019145\n",
      "Train Loss 2500 : 0.5839416466110416\n",
      "Train Loss 3000 : 0.5844537771438205\n",
      "Train Loss 3500 : 0.6001840460488466\n",
      "Train Loss 4000 : 0.5938078660712994\n",
      "Train Loss 4500 : 0.5908756368025687\n",
      "Train Loss 5000 : 0.5904852555460604\n",
      "Train Loss 5500 : 0.5916502535613374\n",
      "Train Loss 6000 : 0.5921885188570573\n",
      "Train Loss 6500 : 0.59186182102328\n",
      "Train Loss 7000 : 0.5913806774227408\n",
      "Train Loss 7500 : 0.5960487784306773\n",
      "Train Loss 8000 : 0.5959353433406743\n",
      "Train Loss 8500 : 0.5964447757904722\n",
      "Train Loss 9000 : 0.5990811839541231\n",
      "Train Loss 9500 : 0.6000669560532512\n",
      "Train Loss 10000 : 0.6011606796807872\n",
      "Train Loss 10500 : 0.6022758753561409\n",
      "Train Loss 11000 : 0.6023304967667528\n",
      "Train Loss 11500 : 0.6024131444702293\n",
      "Train Loss 12000 : 0.6029205524147118\n",
      "Train Loss 12500 : 0.604490465496852\n",
      "Train Loss 13000 : 0.6048086376856583\n",
      "Train Loss 13500 : 0.6062414704515517\n",
      "Train Loss 14000 : 0.6069603429118907\n",
      "Epoch 25 - Val Loss: 0.4837417842640096, bn_score Val Accuracy: 0.9623346800649194, cl_score Val Accuracy: 0.8744777098656722\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_25.pth\n",
      "Train Loss 0 : 0.8663233518600464\n",
      "Train Loss 500 : 0.5583280698654657\n",
      "Train Loss 1000 : 0.557363059085149\n",
      "Train Loss 1500 : 0.5743678361289128\n",
      "Train Loss 2000 : 0.583166911577185\n",
      "Train Loss 2500 : 0.5846837322607112\n",
      "Train Loss 3000 : 0.5820384246760515\n",
      "Train Loss 3500 : 0.5849137677968301\n",
      "Train Loss 4000 : 0.5819389882065231\n",
      "Train Loss 4500 : 0.5834149364309609\n",
      "Train Loss 5000 : 0.5818338655581569\n",
      "Train Loss 5500 : 0.583022985379507\n",
      "Train Loss 6000 : 0.5830313492582798\n",
      "Train Loss 6500 : 0.5852939603976169\n",
      "Train Loss 7000 : 0.5862879475052277\n",
      "Train Loss 7500 : 0.5886760039852604\n",
      "Train Loss 8000 : 0.588493175456053\n",
      "Train Loss 8500 : 0.5892070313874965\n",
      "Train Loss 9000 : 0.5907605773137745\n",
      "Train Loss 9500 : 0.5903147301109661\n",
      "Train Loss 10000 : 0.5915497751328127\n",
      "Train Loss 10500 : 0.5923643198640588\n",
      "Train Loss 11000 : 0.5941468849496687\n",
      "Train Loss 11500 : 0.5946416172402043\n",
      "Train Loss 12000 : 0.5964661938326573\n",
      "Train Loss 12500 : 0.5974342350658939\n",
      "Train Loss 13000 : 0.5998101070979647\n",
      "Train Loss 13500 : 0.600255438437214\n",
      "Train Loss 14000 : 0.6016370054783999\n",
      "Epoch 26 - Val Loss: 0.48467284181739023, bn_score Val Accuracy: 0.963595082703132, cl_score Val Accuracy: 0.8742618874961152\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_26.pth\n",
      "Train Loss 0 : 0.12687775492668152\n",
      "Train Loss 500 : 0.5688386082206807\n",
      "Train Loss 1000 : 0.5527977100829338\n",
      "Train Loss 1500 : 0.5533259165672453\n",
      "Train Loss 2000 : 0.562995139489426\n",
      "Train Loss 2500 : 0.5668568361438111\n",
      "Train Loss 3000 : 0.5761565989571084\n",
      "Train Loss 3500 : 0.5750465032861906\n",
      "Train Loss 4000 : 0.5733082791603217\n",
      "Train Loss 4500 : 0.5728970297389403\n",
      "Train Loss 5000 : 0.5759251192994259\n",
      "Train Loss 5500 : 0.5794538375727698\n",
      "Train Loss 6000 : 0.580086102792436\n",
      "Train Loss 6500 : 0.5807261563218795\n",
      "Train Loss 7000 : 0.5793594587596266\n",
      "Train Loss 7500 : 0.5812314007354413\n",
      "Train Loss 8000 : 0.5813635664498727\n",
      "Train Loss 8500 : 0.5814723874051827\n",
      "Train Loss 9000 : 0.5839859767952091\n",
      "Train Loss 9500 : 0.5855928133385927\n",
      "Train Loss 10000 : 0.5850558060689037\n",
      "Train Loss 10500 : 0.587243884134436\n",
      "Train Loss 11000 : 0.5861890156951123\n",
      "Train Loss 11500 : 0.586593453212285\n",
      "Train Loss 12000 : 0.588270297123287\n",
      "Train Loss 12500 : 0.5897848145593291\n",
      "Train Loss 13000 : 0.5915917691746514\n",
      "Train Loss 13500 : 0.5933159695615808\n",
      "Train Loss 14000 : 0.5932446686945695\n",
      "Epoch 27 - Val Loss: 0.47976765310941044, bn_score Val Accuracy: 0.9638281708622536, cl_score Val Accuracy: 0.8764460098760316\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_27.pth\n",
      "Train Loss 0 : 0.4883573651313782\n",
      "Train Loss 500 : 0.5375255774873906\n",
      "Train Loss 1000 : 0.5568614698579901\n",
      "Train Loss 1500 : 0.5588546599975621\n",
      "Train Loss 2000 : 0.5554945982274958\n",
      "Train Loss 2500 : 0.5592442873198061\n",
      "Train Loss 3000 : 0.5686704126422215\n",
      "Train Loss 3500 : 0.5683498588044561\n",
      "Train Loss 4000 : 0.5691304412437428\n",
      "Train Loss 4500 : 0.568707483479415\n",
      "Train Loss 5000 : 0.5683713517129612\n",
      "Train Loss 5500 : 0.5676420851401685\n",
      "Train Loss 6000 : 0.570795844480134\n",
      "Train Loss 6500 : 0.5755150005503729\n",
      "Train Loss 7000 : 0.5775268316151724\n",
      "Train Loss 7500 : 0.5760887578261253\n",
      "Train Loss 8000 : 0.578738077428963\n",
      "Train Loss 8500 : 0.5812782500099263\n",
      "Train Loss 9000 : 0.5831842838266922\n",
      "Train Loss 9500 : 0.5865023441196806\n",
      "Train Loss 10000 : 0.5869899209592584\n",
      "Train Loss 10500 : 0.586725002892999\n",
      "Train Loss 11000 : 0.5875184664501639\n",
      "Train Loss 11500 : 0.5886341330818651\n",
      "Train Loss 12000 : 0.5911570457133903\n",
      "Train Loss 12500 : 0.5908429807806477\n",
      "Train Loss 13000 : 0.5919283822692962\n",
      "Train Loss 13500 : 0.5926650448369921\n",
      "Train Loss 14000 : 0.5934278861018606\n",
      "Epoch 28 - Val Loss: 0.4710117902518181, bn_score Val Accuracy: 0.9639403984944231, cl_score Val Accuracy: 0.8751338098691254\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_28.pth\n",
      "Train Loss 0 : 0.10155992209911346\n",
      "Train Loss 500 : 0.5545740300824795\n",
      "Train Loss 1000 : 0.5303585082878703\n",
      "Train Loss 1500 : 0.5419879383224495\n",
      "Train Loss 2000 : 0.5427762928443483\n",
      "Train Loss 2500 : 0.5463759152591824\n",
      "Train Loss 3000 : 0.55282322919044\n",
      "Train Loss 3500 : 0.5579928862163996\n",
      "Train Loss 4000 : 0.5682440779579934\n",
      "Train Loss 4500 : 0.5728298550882596\n",
      "Train Loss 5000 : 0.5731318322762418\n",
      "Train Loss 5500 : 0.5782076039575762\n",
      "Train Loss 6000 : 0.5804182510239713\n",
      "Train Loss 6500 : 0.5826356250815592\n",
      "Train Loss 7000 : 0.581019330371594\n",
      "Train Loss 7500 : 0.5808051263182149\n",
      "Train Loss 8000 : 0.5800605170711781\n",
      "Train Loss 8500 : 0.5796065767295494\n",
      "Train Loss 9000 : 0.58166032318899\n",
      "Train Loss 9500 : 0.582193945729962\n",
      "Train Loss 10000 : 0.5832971936924439\n",
      "Train Loss 10500 : 0.5851425911439703\n",
      "Train Loss 11000 : 0.585895785178117\n",
      "Train Loss 11500 : 0.5861054007843992\n",
      "Train Loss 12000 : 0.5866540083526839\n",
      "Train Loss 12500 : 0.5882004937350921\n",
      "Train Loss 13000 : 0.5893649488214762\n",
      "Train Loss 13500 : 0.5899253181405453\n",
      "Train Loss 14000 : 0.5918570251428605\n",
      "Epoch 29 - Val Loss: 0.4700240472855094, bn_score Val Accuracy: 0.9638281708622536, cl_score Val Accuracy: 0.878163955937705\n",
      "Model saved at ./saved_model/review_classifier_BATCH_8_epoch_29.pth\n",
      "평가 결과가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
